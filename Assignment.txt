#Install Spark
#Install Kafka
#Give Paths in the Environmental Variables

#Start Zookeeper:
zookeeper-server-start.bat config\zookeeper.properties
#Start Kafka server:
kafka-server-start.bat config\server.properties
#Create a Kafka topic named:9092 --topic events1
#You'll need confluent_kafka package events1:
kafka-topics.bat --create --bootstrap-server localhost to interact with Kafka:
pip install confluent_kafka
#Install Required Packages:
pip install pyspark delta-spark kafka

#Navigate to the Directory Where You Want to Create the Maven Project:

cd path\to\desired\directory
mvn archetype:generate
cd your-project-name

#It will create one project in that project directory we have to add pom.xml file 

<!-- Spark Dependencies -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.12</artifactId>
        <version>3.5.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming_2.12</artifactId>
        <version>3.5.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
        <version>3.5.0</version>
    </dependency>
    
    <!-- Kafka Dependencies -->
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>3.6.1</version>
    </dependency>

    <!-- Delta Lake Dependency -->
    <dependency>
        <groupId>io.delta</groupId>
        <artifactId>delta-core_2.12</artifactId>
        <version>1.0.0</version>
    </dependency>
    <dependency>
        <groupId>io.delta</groupId>
        <artifactId>delta-spark_2.12</artifactId>
        <version>3.1.0</version>
    </dependency>
    </dependencies>
    <build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-resources-plugin</artifactId>
            <version>3.2.0</version>
            <executions>
                <execution>
                    <id>copy-resources</id>
                    <phase>validate</phase>
                    <goals>
                        <goal>copy-resources</goal>
                    </goals>
                    <configuration>
                        <outputDirectory>${basedir}/target/classes</outputDirectory>
                        <resources>
                            <resource>
                                <directory>src/main/python</directory>
                            </resource>
                        </resources>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
</project>

#Add these dependencies to the pom.xml file to establish the connection between the spark,kafka and Delta lake
#save the python files in the project
#kafka_producer.py

from confluent_kafka import Producer
import json
import time

# Kafka broker address
bootstrap_servers = 'localhost:9092'
# Kafka topic
topic = 'events'

def delivery_report(err, msg):
    """ Called once for each message produced to indicate delivery result.
        Triggered by poll() or flush(). """
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

def produce_events(producer, num_records):
    for i in range(num_records):
        event = {
            "Event Name": "Event{}".format(i+1),
            "Event Type": "Type{}".format(i+1),
            "Event Value": "Value{}".format(i+1),
            "Event Timestamp": time.time(),
            "Event Page Source": "Source{}".format(i+1),
            "Event Page URL": "URL{}".format(i+1),
            "Event Component ID": "Component{}".format(i+1),
            "user_id": "User{}".format(i+1),
            "Event Date": time.strftime('%Y-%m-%d %H:%M:%S')
        }
        # Produce message to Kafka topic
        producer.produce(topic, json.dumps(event), callback=delivery_report)
    # Wait up to 1 second for events to be produced
    producer.flush(1)

if __name__ == '__main__':
    # Create Kafka producer instance
    producer_conf = {'bootstrap.servers': bootstrap_servers}
    producer = Producer(producer_conf)

    try:
        # Produce 10 events
        produce_events(producer, 10)
    except KeyboardInterrupt:
        pass
    finally:
        # Clean up resources
        producer.flush()

#spark_consumer.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, expr
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Define Kafka topic to subscribe to
kafka_topic = 'events'
# Define Delta Lake path to store the data
delta_path = 'C:\Apache\maven\bin\kafka-spark-delta'

def main():
    # Initialize SparkSession
    spark = SparkSession.builder \
        .appName("Kafka Spark Delta Consumer") \
        .getOrCreate()

    # Define schema for the events
    event_schema = StructType([
        StructField("Event Name", StringType()),
        StructField("Event Type", StringType()),
        StructField("Event Value", StringType()),
        StructField("Event Timestamp", DoubleType()),
        StructField("Event Page Source", StringType()),
        StructField("Event Page URL", StringType()),
        StructField("Event Component ID", StringType()),
        StructField("user_id", StringType()),
        StructField("Event Date", StringType())
    ])

    # Define Kafka options
    kafka_options = {
        "kafka.bootstrap.servers": "localhost:9092",
        "subscribe": kafka_topic,
        "startingOffsets": "earliest"
    }

    # Read data from Kafka topic as DataFrame
    kafka_df = spark \
        .readStream \
        .format("kafka") \
        .options(**kafka_options) \
        .load()

    # Parse JSON data from 'value' column
    parsed_df = kafka_df \
        .selectExpr("CAST(value AS STRING)") \
        .select(from_json(col("value"), event_schema).alias("data")) \
        .select("data.*")

    # Write data to Delta Lake
    delta_sink = parsed_df \
        .writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", "C:\Apache\maven\bin\kafka-spark-delta") \
        .start(delta_path)

    # Await termination
    delta_sink.awaitTermination()

if __name__ == "__main__":
    main()
#in this spark_consumer file delta lake will be created and genarated data will be stored in the delta lake at the given path.
#navigate to kafka_producer.py in the command prompt and type the below command
-> python kafka_producere.py
#start the spark job using spark submit using the maven created .jar file and using both .py files paths and by importing necessary packages 
spark-submit --master spark://user:8080 --deploy-mode cluster --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,C:\Apache\maven\bin\kafka-spark-delta\target\kafka-spark-delta-1.0-SNAPSHOT.jar,C:\Apache\maven\bin\kafka-spark-delta\src\main\python\kafka_producer.py,C:\Apache\maven\bin\kafka-spark-delta\src\main\python\spark_consumer.py
				


